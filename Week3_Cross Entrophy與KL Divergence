Week3_Cross Entrophy與KL Divergence

**兩者定義**

Cross Entrophy(交叉熵) KL Divergence(KL散度，相對熵) 兩者都可作為模型訓練的損失函數。

熵 的定義是 H(P) = −∑P(x)logP(x)

交叉熵 的定義是 H(P,Q) = −∑P(x)logQ(x)

相對熵 的定義是 KL(P∣∣Q) = H(P,Q)−H(P)

其中 P為真實機率分布, Q為模型預測分布

**使用時機**

交叉熵較適合在訓練單標籤分類問題模型作為損失函數, 而相對熵則更適合在知識蒸餾或半監督學習中使用。

但由定義可知 交叉熵 與 相對熵 的差異為 H(P), 也就是真實機率分布的熵

所以若在訓練單標籤分類問題模型, 真實標籤為one-hot編碼(例:[0, 1, 0, 0, 0])時, 因為H(P) = 0

交叉熵 與 相對熵 應該是等價的, 為什麼使用 交叉熵 會比較好呢?

我問chatgpt的結果是:
在單標籤分類問題中，真實標籤已經是 one-hot，這時候 相對熵 和 交叉熵 的額外差異H(P)已經消失，因此直接使用交叉熵更符合標準機器學習優化框架。
1.交叉熵與 Softmax 直接配合，梯度計算簡單且高效。
2.交叉熵的數值穩定性較好，對極端機率值（接近 0 或 1）有較好的處理能力。
3.交叉熵有現成的 GPU 優化實作，在計算效率上更優。
KL 散度的計算通常需要額外的 log 運算，在標準分類問題中，這額外的計算是不必要的，因此交叉熵成為更標準化的選擇。

因此, 雖然在數學上等價, 在單標籤分類問題應用上交叉熵仍然是更好的選擇, 至於真實標籤非one-hot編碼, 則使用相對熵。
